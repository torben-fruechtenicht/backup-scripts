#! /usr/bin/env bash

set -euo pipefail

PATH="$(dirname "$(readlink -e "$0")"):$PATH"

while getopts ":hb:c:ot" opt; do
    case $opt in
        b)
            # TODO check if number
            declare -r BATCH_SIZE=$OPTARG;;
        c )
            # each OPTARG for "c" is expected to be a valid string of find checks
            candidate_checks="${candidate_checks+$candidate_checks} $OPTARG";;
        o )
            declare -r COLLECT_OLD_FILES=;;
        t )
            declare -r TEST_ONLY=
            echo "[WARNING] Dry run! Will only collect candidates without sync" >&2;;
        h )
            echo "[USAGE] sync_dir [-b BATCHSIZE] -c CANDIDATE_CHECKS... [-o] [-t] SOURCEDIR S3TARGET" >&2
            exit;;
        ? ) 
            echo "[ERROR] Unknown parameter \"$OPTARG\"" >&2
            exit 1;;
	esac
done
shift $(expr $OPTIND - 1 )

if [[ -v candidate_checks ]]; then
    declare -r CANDIDATE_CHECKS=$candidate_checks
else 
    echo "[ERROR] No candidate checks given" >&2
    exit 1
fi

declare -r SOURCE_DIRECTORY="$(readlink -e "${1:-}")"
if [[ -z $SOURCE_DIRECTORY ]]; then
    echo "[ERROR] No source directory ${1:-}" >&2
    exit 1 
fi
declare -r TARGET=${2:-}
if ! [[ $TARGET =~ s3://[a-zA-Z0-9\./]+/ ]]; then
    echo "[ERROR] Target $TARGET is no valid s3 url" >&2
    exit 1
fi


print_duration() {
    local mins secs
    ((mins=${SECONDS}/60))
    ((secs=${SECONDS}%60))
    if [[ $mins != 0 ]]; then
        echo "$mins minutes $secs seconds"
    else 
        echo "$secs seconds"
    fi
}

collect_older_candidates() {
    local s3cmd_out s3cmd_rc

    s3cmd_out=$(s3cmd -r ls "$TARGET" 2>&1)
    s3cmd_rc=$?

    if ! [[ $s3cmd_rc = 0 ]]; then
        echo -e "[ERROR] s3cmd ls call failed with return code $s3cmd_rc:\n$s3cmd_out" >&2
        return 1
    fi

    local -r synced_list=$(mktemp)
    sed -r 's|([0-9-]+ [0-9:]+)[[:space:]]+([0-9]+)[[:space:]]+'"$TARGET"'/(.+)|\3|' <<<"$s3cmd_out" > "$synced_list"
    find "$SOURCE_DIRECTORY" -type f ${CANDIDATE_CHECKS+$CANDIDATE_CHECKS} \
        ! -newer "$LAST_RUN_TIMESTAMP" -printf '%P\n' |\
        while read file; do
            grep -q "$file" "$synced_list" || echo "$file"
        done
    rm "$synced_list"
}

collect_newer_candidates() {
    find "$SOURCE_DIRECTORY" -type f ${CANDIDATE_CHECKS+$CANDIDATE_CHECKS} \
        -newer "$LAST_RUN_TIMESTAMP" -printf '%P\n'
}

get_last_run_ts_file()  {
    local -r ts_file_dir=$(readlink -m "$HOME/.local/share/backup-scripts")
    if ! [[ -e $ts_file_dir ]]; then
        mkdir --parents "$ts_file_dir"
    fi
    local -r ts_file="$ts_file_dir/last_run_ts"
    if ! [[ -e $ts_file ]]; then
        touch "$ts_file"
    fi
    echo "$ts_file"
}

sync_files() {
    local -r source_dir=$1
    local -r files_to_sync=$2
    local -r target_url=$3

    # about "--exclude" - to restrict to only the files we pass via "--files-from", we must first exclude all
    # about "--no-check-md5" - s3cmd will compare found files that are older than $INITIAL_SYNC_START_TSFILE with uploaded once
    #   and maybe wrongly detect "identical" files (identity based on md5 checksums). (this might also make things faster because 
    #   checksums must not be produced/fetched). From: https://stackoverflow.com/a/28188553/1295519
    local -r opts="--no-check-md5 --acl-private --no-progress --skip-existing --preserve --exclude '*'"
    local s3cmd_out s3cmd_rc
    s3cmd_out=$(s3cmd $opts --files-from - sync "$source_dir" "$target_url" <<<"$files_to_sync" 2>&1)
    s3cmd_rc=$?

    if ! [[ $s3cmd_rc = 0 ]]; then
        echo -e "[ERROR] s3cmd sync call failed with return code $s3cmd_rc:\n$s3cmd_out" >&2
        return 1
    fi

    sed -r -n 's/^upload: \x27(.+)\x27 -> \x27s3:.+\x27.*/\1/p' <<<"$s3cmd_out"
}



declare -r LAST_RUN_TIMESTAMP=$(get_last_run_ts_file)

if [[ -v COLLECT_OLD_FILES ]]; then
    echo "[INFO] Collecting candidates from before last run" >&2
    declare -r CANDIDATES=$(collect_older_candidates | sort)
else
    declare -r CANDIDATES=$(collect_newer_candidates | sort)
fi 

if [[ -z $CANDIDATES ]]; then
    echo "[INFO] No candidates found for sync" >&2
    exit
fi 

if [[ -v BATCH_SIZE ]]; then
    declare -r SYNC_CANDIDATES=$(head -n "$BATCH_SIZE" <<<"$CANDIDATES")
else    
    declare -r SYNC_CANDIDATES=$CANDIDATES
fi
candidates_count=$(wc -l <<<"$SYNC_CANDIDATES")

if [[ -v TEST_ONLY ]]; then
    echo "[INFO] Found $candidates_count file(s)" >&2
    echo "$SYNC_CANDIDATES"
    exit
fi

synced_files=$(sync_files "$SOURCE_DIRECTORY" "$SYNC_CANDIDATES" "$TARGET")
synced_count=$(wc -l <<<"$synced_files")

# if we synced older files, we should not touch LAST_RUN_TIMESTAMP: I have a fear that this might lead to newer files not picked up for
# syncing...
if ! [[ -v COLLECT_OLD_FILES ]]; then
    touch "$LAST_RUN_TIMESTAMP"    
fi

if [[ -z $synced_files ]]; then
    echo "[WARN] No files were synced but candidates exist: $candidates_count" >&2
    exit 1
fi

if [[ $candidates_count != $synced_count ]]; then
    echo "[WARN] Found $candidates_count candidates but $synced_count were synced" >&2
fi

echo "[INFO] Synced $synced_count file(s) in $(print_duration)" >&2
echo "$synced_files"